{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copia di Experiment_10.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNPwHGOu8cfpRK+ykNune+Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gQEMjV70QkdZ"},"source":["# Introductive code"]},{"cell_type":"code","metadata":{"id":"nPuFOL3vQMC8"},"source":["# Explicitly print the variables: useful for debugging\n","\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","# Import most relevant libraries\n","\n","import os\n","import tensorflow as tf\n","import numpy as np\n","\n","# Import library for handling json files\n","\n","import json\n","\n","# Import library for handling dataframes\n","\n","import pandas as pd\n","\n","# Import library for handling images\n","\n","from PIL import Image\n","\n","# Set the seed for random operations, in order to let all the experiments be reproducible\n","\n","SEED = 1234\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UnWXlOPGQvlk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609530683533,"user_tz":-60,"elapsed":20500,"user":{"displayName":"Andrea Boselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTT0DD5_ctcwk1BCWC99SFDQm1i1GG71SnE5ZXDw=s64","userId":"10139055795145528550"}},"outputId":"9b1b3157-8ff9-483e-d3c1-5cf7e0bb399b"},"source":["# Add Colab (with Drive)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mm1xB5GXQy01"},"source":["# Run this to unzip the folder for the current session\n","\n","!unzip '/content/drive/My Drive/ANNDL_Homeworks3/Experiment_10/anndl-2020-vqa.zip'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ugMWN4QQ1tu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609530877914,"user_tz":-60,"elapsed":214867,"user":{"displayName":"Andrea Boselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTT0DD5_ctcwk1BCWC99SFDQm1i1GG71SnE5ZXDw=s64","userId":"10139055795145528550"}},"outputId":"b6b076ae-2cdd-4c51-caa8-c440e5aadaf5"},"source":["# Inspect the dataset\n","\n","!ls '/content'\n","!ls '/content/VQA_Dataset'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["drive  sample_data  VQA_Dataset\n","Images\ttest_questions.json  train_questions_annotations.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YxyWQZ0vSSPO"},"source":["# We get the current directory cwd ('/content') and the directory env, which contains the zip file and the notebook\n","\n","cwd = os.getcwd()\n","env = '/content/drive/My Drive/ANNDL_Homeworks3/Experiment_10'\n","dataset_dir = os.path.join('/content', 'VQA_Dataset')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tGk4rZR3RD30"},"source":["# Tokenize the words"]},{"cell_type":"code","metadata":{"id":"FmAfNWhzRDlO"},"source":["# From the file .json, extract a dictionary with:\n","\n","# Key: id of the question\n","# Value: infos about the question\n","\n","json_train_q_a_dir = os.path.join(dataset_dir, 'train_questions_annotations.json')\n","with open(json_train_q_a_dir) as json_file:\n","    labels = json.load(json_file)\n","\n","# Manipulate the data with a dataframe\n","\n","df = pd.DataFrame.from_dict(labels, orient='index')\n","df.columns = ['question', 'image_id', 'answer']\n","\n","# Replace the answer \"monkey bars\" with \"monkeybars\"\n","\n","idx = df.index[df['answer'] == 'monkey bars']\n","df.loc[idx,'answer'] = 'monkeybars'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HH7hULoOT9ec"},"source":["# Parameters used in our attempts\n","\n","num_data = 28000\n","MAX_NUM_WORDS = 100000\n","\n","img_h = 64\n","img_w = 64\n","\n","EMBEDDING_SIZE = 64\n","\n","# Import functions we use to convert words to integers\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sGXcZ51RUyhR"},"source":["df = df.head(num_data)\n","\n","# Create tokenizers\n","\n","answer_tokenizer = Tokenizer(num_words= MAX_NUM_WORDS)\n","question_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token = '<UNK>')\n","\n","# Get the list of questions and answers\n","\n","question_list = df['question'].tolist()\n","answer_list = df['answer'].tolist()\n","\n","# Tokenize questions and answers\n","\n","answer_tokenizer.fit_on_texts(answer_list)\n","question_tokenizer.fit_on_texts(question_list)\n","\n","answer_tokenized = answer_tokenizer.texts_to_sequences(answer_list)\n","question_tokenized = question_tokenizer.texts_to_sequences(question_list)\n","\n","# Get the dictionaries of words in questions and answers\n","\n","answer_wtoi = answer_tokenizer.word_index\n","question_wtoi = question_tokenizer.word_index\n","\n","vocabulary_answer_size = len(answer_wtoi) + 1\n","vocabulary_question_size = len(question_wtoi) + 2\n","\n","# Get the maximum length of questions and answers\n","\n","max_answer_length = max(len(sentence) for sentence in answer_tokenized)\n","max_question_length = max(len(sentence) for sentence in question_tokenized)\n","\n","#Â Pad tokenized questions and answers\n","\n","question_encoder_inputs = pad_sequences(question_tokenized, maxlen=max_question_length, padding = 'pre')\n","answer_encoder_inputs = pad_sequences(answer_tokenized, maxlen=max_answer_length, padding = 'post')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IVpvsoUYcqN-"},"source":["# Model definition"]},{"cell_type":"code","metadata":{"id":"nBX2sPG6cpll","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609530886718,"user_tz":-60,"elapsed":199990,"user":{"displayName":"Andrea Boselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTT0DD5_ctcwk1BCWC99SFDQm1i1GG71SnE5ZXDw=s64","userId":"10139055795145528550"}},"outputId":"33f2effd-659f-4ac9-f514-13faaa186cb2"},"source":["### RNN for language input\n","\n","encoder_input = tf.keras.Input(shape=[max_question_length])\n","\n","# Create the embedding representation of input vector of the sentences, passing from {0,1}^N to (0,1)^m smaller representation\n","\n","encoder_embedding_layer = tf.keras.layers.Embedding(input_dim=vocabulary_question_size,       # input dim, is the num of words in the dictionary + 2 (it is not MAX_NUM_WORD because in the dataset there could be less)\n","                                                    output_dim=EMBEDDING_SIZE,                # output dim, is the m, dim of the vector (0,1)^m represetation of the word, it is the m\n","                                                    input_length=max_question_length,         # dimension of a input sequence\n","                                                    mask_zero=True)                           # ignores padding's zeros\n","encoder_embedding_out = encoder_embedding_layer(encoder_input)\n","encoder = tf.keras.layers.LSTM(units=128, return_state=True)\n","#return_sequences: Boolean. Whether to return the last output. in the output sequence, or the full sequence. Default: False.\n","#return_state: Boolean. Whether to return the last state in addition to the output. Default: False.\n","\n","encoded_question, _, _ = encoder(encoder_embedding_out)\n","\n","\n","### CNN for image input\n","\n","# Load VGG16 Model and set training options\n","\n","vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n","vgg.trainable = False\n","\n","# Build the CNN\n","\n","vision_model = tf.keras.Sequential()\n","vision_model.add(vgg)\n","vision_model.add(tf.keras.layers.Flatten())\n","vision_model.add(tf.keras.layers.Dense(128, activation='relu'))\n","\n","image_input = tf.keras.Input(shape=(img_h, img_w, 3))\n","encoded_image = vision_model(image_input)\n","\n","\n","### Combine the 2 models\n","\n","from tensorflow.keras.models import Model\n","\n","merged = tf.keras.layers.concatenate([encoded_question, encoded_image])\n","output = tf.keras.layers.Dense(vocabulary_answer_size, activation='softmax')(merged)\n","vqa_model = Model(inputs=[image_input, encoder_input], outputs=output)\n","\n","### Model summary\n","\n","vqa_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 0s 0us/step\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 21)]         0                                            \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, 21, 64)       222464      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n","__________________________________________________________________________________________________\n","lstm (LSTM)                     [(None, 128), (None, 98816       embedding[0][0]                  \n","__________________________________________________________________________________________________\n","sequential (Sequential)         (None, 128)          14976960    input_3[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 256)          0           lstm[0][0]                       \n","                                                                 sequential[0][0]                 \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 59)           15163       concatenate[0][0]                \n","==================================================================================================\n","Total params: 15,313,403\n","Trainable params: 598,715\n","Non-trainable params: 14,714,688\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mYlF2XEedN2-"},"source":["# Image preprocessing"]},{"cell_type":"code","metadata":{"id":"v4B5o4uucyGJ"},"source":["# Function that loads the image and processes it with VGG16 preprocessing function\n","\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","\n","def load_and_process_image(image_path):\n","  im = Image.open(image_path)\n","  im = im.resize((img_h, img_w), resample=Image.ANTIALIAS)\n","  im = np.array(im)\n","  im = im[:,:,0:3]\n","  return preprocess_input(im)\n","\n","# Function that creates the proper numpy array for storing the images\n","\n","def read_images(paths):\n","  ims = np.zeros((num_data, img_h, img_w, 3))\n","  i = 0\n","  for image_path in paths:\n","    ims[i,:,:,:] = load_and_process_image(image_path)\n","    i += 1\n","  return ims\n","\n","# Use the above functions to create the numpy array for the training\n","\n","image_list = df['image_id'].tolist()\n","images_dir = os.path.join(dataset_dir, 'Images')\n","image_list = [os.path.join(images_dir, im + '.png') for im in image_list]\n","\n","train_X_images = read_images(image_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p7M2GNV4dRIm"},"source":["# Model training"]},{"cell_type":"code","metadata":{"id":"_lzNXy2nc0QL"},"source":["# Set optimization params\n","\n","# Loss\n","ls = tf.keras.losses.CategoricalCrossentropy()\n","\n","# Learning rate\n","lr = 1e-3\n","optim = tf.keras.optimizers.Adam(learning_rate=lr)\n","\n","# Validation metrics\n","val_metric = ['accuracy']\n","\n","# Compile model\n","vqa_model.compile(optimizer=optim, loss=ls, metrics=val_metric)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJ9FcSDKw_Og"},"source":["# We set the proper callbacks for training\n","\n","# Create the folder for the experiments\n","\n","from datetime import datetime\n","\n","cwd = os.getcwd()\n","\n","exps_dir = os.path.join(env, 'vqa_experiments')\n","if not os.path.exists(exps_dir):\n","    os.makedirs(exps_dir)\n","\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","exp_name = 'vqa_exp'\n","\n","exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n","if not os.path.exists(exp_dir):\n","    os.makedirs(exp_dir)\n","    \n","callbacks = []\n","\n","# Model checkpoints\n","\n","ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","if not os.path.exists(ckpt_dir):\n","    os.makedirs(ckpt_dir)\n","\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'),\n","                                                   save_best_only=False, \n","                                                   save_weights_only=True)  # False to save the model directly\n","callbacks.append(ckpt_callback)\n","\n","# Visualize Learning on Tensorboard\n","\n","tb_dir = os.path.join(exp_dir, 'tb_logs')\n","if not os.path.exists(tb_dir):\n","    os.makedirs(tb_dir)\n","    \n","# By default shows losses and metrics for both training and validation\n","\n","tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n","                                             profile_batch=0,\n","                                             histogram_freq=1)  # if 1 shows weights histograms\n","callbacks.append(tb_callback)\n","\n","# Early Stopping\n","\n","early_stop = True\n","if early_stop:\n","    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6)\n","    callbacks.append(es_callback)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZqMuwxic3cw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609537474831,"user_tz":-60,"elapsed":6771274,"user":{"displayName":"Andrea Boselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTT0DD5_ctcwk1BCWC99SFDQm1i1GG71SnE5ZXDw=s64","userId":"10139055795145528550"}},"outputId":"8b524eac-4e8a-4194-de3b-5e869893799a"},"source":["# Convert the answers into categorical\n","\n","from tensorflow.keras.utils import to_categorical\n","y_train = to_categorical(answer_encoder_inputs)\n","\n","# Training parameters\n","\n","num_epochs = 20\n","bs = 1\n","val_split = 0.1\n","\n","# Fit the model\n","\n","vqa_model.fit(x=[train_X_images, question_encoder_inputs],\n","              y=y_train,\n","              epochs=num_epochs,\n","              batch_size = bs,\n","              validation_split=val_split,\n","              shuffle=True,\n","              callbacks=callbacks)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","25200/25200 [==============================] - 633s 25ms/step - loss: 1.9854 - accuracy: 0.4221 - val_loss: 1.1441 - val_accuracy: 0.5468\n","Epoch 2/20\n","25200/25200 [==============================] - 620s 25ms/step - loss: 1.0915 - accuracy: 0.5760 - val_loss: 1.0584 - val_accuracy: 0.5696\n","Epoch 3/20\n","25200/25200 [==============================] - 621s 25ms/step - loss: 0.9543 - accuracy: 0.6295 - val_loss: 1.0363 - val_accuracy: 0.5854\n","Epoch 4/20\n","25200/25200 [==============================] - 629s 25ms/step - loss: 0.8620 - accuracy: 0.6625 - val_loss: 1.0304 - val_accuracy: 0.5971\n","Epoch 5/20\n","25200/25200 [==============================] - 631s 25ms/step - loss: 0.7866 - accuracy: 0.6932 - val_loss: 1.0380 - val_accuracy: 0.5975\n","Epoch 6/20\n","25200/25200 [==============================] - 621s 25ms/step - loss: 0.7430 - accuracy: 0.7044 - val_loss: 1.0844 - val_accuracy: 0.5925\n","Epoch 7/20\n","25200/25200 [==============================] - 621s 25ms/step - loss: 0.6950 - accuracy: 0.7268 - val_loss: 1.1017 - val_accuracy: 0.5896\n","Epoch 8/20\n","25200/25200 [==============================] - 618s 25ms/step - loss: 0.6426 - accuracy: 0.7462 - val_loss: 1.1549 - val_accuracy: 0.5950\n","Epoch 9/20\n","25200/25200 [==============================] - 616s 24ms/step - loss: 0.6080 - accuracy: 0.7622 - val_loss: 1.1996 - val_accuracy: 0.5857\n","Epoch 10/20\n","25200/25200 [==============================] - 613s 24ms/step - loss: 0.5843 - accuracy: 0.7713 - val_loss: 1.2661 - val_accuracy: 0.5929\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f3342019fd0>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"YIWAZ21YxBoQ"},"source":["%load_ext tensorboard\n","%tensorboard --logdir '/content/drive/My Drive/ANNDL_Homeworks3/Experiment_10/vqa_experiments'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JWf3aAxudUUM"},"source":["# Predictions creation"]},{"cell_type":"code","metadata":{"id":"wHgP--y0FDFu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609537652635,"user_tz":-60,"elapsed":759,"user":{"displayName":"Andrea Boselli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTT0DD5_ctcwk1BCWC99SFDQm1i1GG71SnE5ZXDw=s64","userId":"10139055795145528550"}},"outputId":"adf591c8-b97a-4fd6-a727-cb6fb3deb8a4"},"source":["# Load the weights of the chosen model\n","\n","vqa_model.load_weights(os.path.join(env,'vqa_experiments', 'vqa_exp_Jan01_20-00-36', 'ckpts', 'cp_04.ckpt'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f333a2f78d0>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"EG3-aRCPsvxT"},"source":["## Create the proper dictionaries\n","\n","# Inverse dictionary associated with our dictionary answer_wtoi\n","\n","answer_itow = {v:k for k, v in answer_wtoi.items()}\n","\n","# Given dictionary to be used for the submission (with 'monkey bars' modified in 'monkeybars)\n","\n","labels_dict = {\n","        '0': 0,\n","        '1': 1,\n","        '2': 2,\n","        '3': 3,\n","        '4': 4,\n","        '5': 5,\n","        'apple': 6,\n","        'baseball': 7,\n","        'bench': 8,\n","        'bike': 9,\n","        'bird': 10,\n","        'black': 11,\n","        'blanket': 12,\n","        'blue': 13,\n","        'bone': 14,\n","        'book': 15,\n","        'boy': 16,\n","        'brown': 17,\n","        'cat': 18,\n","        'chair': 19,\n","        'couch': 20,\n","        'dog': 21,\n","        'floor': 22,\n","        'food': 23,\n","        'football': 24,\n","        'girl': 25,\n","        'grass': 26,\n","        'gray': 27,\n","        'green': 28,\n","        'left': 29,\n","        'log': 30,\n","        'man': 31,\n","        'monkeybars': 32,\n","        'no': 33,\n","        'nothing': 34,\n","        'orange': 35,\n","        'pie': 36,\n","        'plant': 37,\n","        'playing': 38,\n","        'red': 39,\n","        'right': 40,\n","        'rug': 41,\n","        'sandbox': 42,\n","        'sitting': 43,\n","        'sleeping': 44,\n","        'soccer': 45,\n","        'squirrel': 46,\n","        'standing': 47,\n","        'stool': 48,\n","        'sunny': 49,\n","        'table': 50,\n","        'tree': 51,\n","        'watermelon': 52,\n","        'white': 53,\n","        'wine': 54,\n","        'woman': 55,\n","        'yellow': 56,\n","        'yes': 57\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7fvXYlEvj4y"},"source":["# Load the .json test dictionary\n","\n","json_test_q_dir = os.path.join(dataset_dir, 'test_questions.json')\n","with open(json_test_q_dir) as json_file:\n","    test_questions_dict = json.load(json_file)\n","\n","results = {}\n","\n","for key in test_questions_dict.keys():\n","\n","  # Process the question\n","\n","  question = test_questions_dict[key]['question']\n","  question_t = question_tokenizer.texts_to_sequences([question])\n","  question_p = pad_sequences(question_t, maxlen=max_question_length, padding = 'pre', truncating = 'pre')\n","\n","  # Process the image\n","\n","  image_name = test_questions_dict[key]['image_id']\n","  image_path = os.path.join(dataset_dir,'Images',image_name + '.png')\n","\n","  test_image = np.zeros((1,img_h,img_w,3))\n","  test_image[0,:,:,:] = load_and_process_image(image_path)\n","\n","  # Generate the prediction\n","\n","  pred = vqa_model.predict(x=[test_image, question_p])\n","  pred_id = np.array(tf.argmax(pred,-1))\n","  pred_word = answer_itow[pred_id[0]]\n","  pred_label = labels_dict[pred_word]\n","\n","  # Save the prediction\n","\n","  results[key] = pred_label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oRM6Ek1wskxO"},"source":["# Required function for csv creation\n","\n","import os\n","from datetime import datetime\n","\n","def create_csv(results, results_dir='./'):\n","\n","    csv_fname = 'results_'\n","    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n","\n","    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n","\n","        f.write('Id,Category\\n')\n","\n","        for key, value in results.items():\n","            f.write(key + ',' + str(value) + '\\n')\n","\n","# Use the function to create the csv\n","\n","create_csv(results, results_dir = env)"],"execution_count":null,"outputs":[]}]}